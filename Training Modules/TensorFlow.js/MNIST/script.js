// Use this code as a future template for all CNN tasks...
// You are not that much of an expert in JavaScript to be able to
// write EVERYTHING on your own yet.

// See how you import a class from a local file?
import { MnistData } from './data.js'

console.log('Hello TensorFlow')

async function showExamples (data) {
  // Create a container in the visor
  const surface =
  tfvis.visor().surface({ name: 'Input Data Examples', tab: 'Input Data' })

  // Get the examples
  const examples = data.nextTestBatch(20)
  const numExamples = examples.xs.shape[0]

  // Create a canvas element to render each example
  for (let i = 0 i < numExamples i++) {
    const imageTensor = tf.tidy(() => {
      // Reshape the image to 28x28 px
      return examples.xs
        .slice([i, 0], [1, examples.xs.shape[1]])
        .reshape([28, 28, 1])
    })

    const canvas = document.createElement('canvas')
    canvas.width = 28
    canvas.height = 28
    canvas.style = 'margin: 4px'
    await tf.browser.toPixels(imageTensor, canvas)
    surface.drawArea.appendChild(canvas)

    imageTensor.dispose()
  }
}

/**
 * The data representation used here for the labels is called
 * one-hot encoding and is common in classification problems.
 * Each class has a probability associated with it for each example.
 * When we know exactly what it should be,
 * we can set that probability to 1 and the others to 0.
 */

function getModel () {
  // Initialize training module
  const model = tf.sequential()

  const IMAGE_WIDTH = 28
  const IMAGE_HEIGHT = 28
  const IMAGE_CHANNELS = 1

  // In the first layer of our convolutional neural network we have
  // to specify the input shape. Then we specify some parameters for
  // the convolution operation that takes place in this layer.
  model.add(tf.layers.conv2d({
    inputShape: [IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS],
    kernelSize: 5, // try 3?
    filters: 8,
    // The "step size" of the sliding window—i.e., 
    // how many pixels the filter will shift each time it moves over the image.
    strides: 1,
    // https://developers.google.com/machine-learning/glossary/#ReLU
    activation: 'relu', 
    // The method to use for randomly initializing the model weights,
    // which is very important to training dynamics.
    kernelInitializer: 'varianceScaling'
  }))

  // The MaxPooling layer acts as a sort of downsampling using max values
  // in a region instead of averaging.
  model.add(tf.layers.maxPooling2d({ poolSize: [2, 2], strides: [2, 2] }))

  // Repeat another conv2d + maxPooling stack.
  // Note that we have more filters in the convolution.
  model.add(tf.layers.conv2d({
    kernelSize: 5,
    filters: 16,
    strides: 1,
    activation: 'relu',
    kernelInitializer: 'varianceScaling'
  }))
  model.add(tf.layers.maxPooling2d({ poolSize: [2, 2], strides: [2, 2] }))

  // Now we flatten the output from the 2D filters into a 1D vector to prepare
  // it for input into our last layer. This is common practice when feeding
  // higher dimensional data to a final classification output layer.
  model.add(tf.layers.flatten())

  // Our last layer is a dense layer which has 10 output units, one for each
  // output class (i.e. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9).
  const NUM_OUTPUT_CLASSES = 10
  model.add(tf.layers.dense({
    units: NUM_OUTPUT_CLASSES,
    kernelInitializer: 'varianceScaling',
    // compute probability distributions over the 10 possible classes
    activation: 'softmax'
  }))

  // Choose an optimizer, loss function and accuracy metric,
  // then compile and return the model
  const optimizer = tf.train.adam()
  model.compile({
    optimizer: optimizer,
    /*
     * categoricalCrossentropy measures the error between
     * the probability distribution generated by the last layer of our model
     * and the probability distribution given by our true label.
     */
    loss: 'categoricalCrossentropy',
    metrics: ['accuracy']
  })

  return model
}

async function train(model, data) {
  /*
  * Here we decide which metrics we are going to monitor.
  * We will monitor loss and accuracy on the training set
  * as well as loss and accuracy on the validation set
  * (val_loss and val_acc, respectively).
  *
  * When using the Layers API loss and accuracy
  * is computed on each batch and epoch.
  */
  const metrics = ['loss', 'val_loss', 'acc', 'val_acc']
  const container = {
    name: 'Model Training', tab: 'Model', styles: { height: '1000px' }
  }
  // For visualization
  const fitCallbacks = tfvis.show.fitCallbacks(container, metrics)

  /*
  * trainDataSize is set to 5500 and testDataSize to 1000
  * to make it faster for you to experiment with.
  * Once you've got this tutorial running, feel free to increase that to
  * 55000 and 10000, respectively. It will take a bit longer to train
  * but should still work in the browser on many machines.
  */
  const BATCH_SIZE = 512
  const TRAIN_DATA_SIZE = 5500
  const TEST_DATA_SIZE = 1000

  const [trainXs, trainYs] = tf.tidy(() => {
    // Load the next training batch
    const d = data.nextTrainBatch(TRAIN_DATA_SIZE)
    return [
      d.xs.reshape([TRAIN_DATA_SIZE, 28, 28, 1]),
      d.labels
    ]
  })

  const [testXs, testYs] = tf.tidy(() => {
    // Load the next testing batch
    const d = data.nextTestBatch(TEST_DATA_SIZE)
    return [
      d.xs.reshape([TEST_DATA_SIZE, 28, 28, 1]),
      d.labels
    ]
  })

  return model.fit(trainXs, trainYs, {
    batchSize: BATCH_SIZE,
    /*
     * If we do well on our training data but not on our validation data,
     * it means the model is likely overfitting to the training data
     * and won't generalize well to input it has not previously seen.
     */
    validationData: [testXs, testYs],
    epochs: 10,
    shuffle: true,
    callbacks: fitCallbacks
  })
}

const classNames = [
  'Zero', 'One', 'Two', 'Three', 'Four', 
  'Five', 'Six', 'Seven', 'Eight', 'Nine'
]

/**
 * Take 500 images and predict what digit is in them.
 *
 * You may also notice that we can do predictions on all 500 examples at once. 
 * This is the power of vectorization that TensorFlow.js provides.
 */

function doPrediction(model, data, testDataSize = 500) {

  const IMAGE_WIDTH = 28
  const IMAGE_HEIGHT = 28
  const testData = data.nextTestBatch(testDataSize)
  const testxs = testData.xs.reshape([testDataSize, IMAGE_WIDTH, IMAGE_HEIGHT, 1])

  /*
   * Notably the argmax function is what gives us the index of
   * the highest probability class. Remember that the model outputs 
   * a probability for each class. Here we find out the highest probability
   * and assign that as the prediction.
   * 
   * We do not use any probability threshold here. 
   * We take the highest value even if it is relatively low. 
   * An interesting extension to this project would be to
   * set some required minimum probability and indicate ‘no digit found' 
   * if no class meets this classification threshold.
   */
  const labels = testData.labels.argMax(-1)
  const preds = model.predict(testxs).argMax(-1)

  testxs.dispose()
  return [preds, labels]
}

/** With a set of predictions and labels we can calculate accuracy for each class. */
async function showAccuracy(model, data) {
  const [preds, labels] = doPrediction(model, data)
  // No fancy algebra here -- just use the functions provided by tfjs
  const classAccuracy = await tfvis.metrics.perClassAccuracy(labels, preds)
  const container = {name: 'Accuracy', tab: 'Evaluation'}
  tfvis.show.perClassAccuracy(container, classAccuracy, classNames)

  labels.dispose()
}

/**
 * A confusion matrix is similar to per class accuracy 
 * but further breaks it down to show patterns of misclassification. 
 * It allows you to see if the model is getting confused about 
 * any particular pairs of classes.
 */
async function showConfusion(model, data) {
  const [preds, labels] = doPrediction(model, data)
  const confusionMatrix = await tfvis.metrics.confusionMatrix(labels, preds)
  const container = { name: 'Confusion Matrix', tab: 'Evaluation' }
  tfvis.render.confusionMatrix(container, {values: confusionMatrix, tickLabels: classNames})

  labels.dispose()
}

async function run () {
  const data = new MnistData()
  // Use await to call async functions
  await data.load()
  await showExamples(data)
  const model = getModel()
  // For visualization
  tfvis.show.modelSummary(
    {
      // These strings don't define the exact titles of the graph.
      // Rather, they represent their MEANINGS
      name: 'Model Architecture',
      tab: 'Model'
    }, model
  )
  await train(model, data)
  // Prediction
  await showAccuracy(model, data);
  await showConfusion(model, data);
}

document.addEventListener('DOMContentLoaded', run)
